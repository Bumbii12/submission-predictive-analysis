# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZQDczKdifT5UNlBlc0pY9YrSHIcZpczI

# Proyek Analisis Sentimen: [Trip Advisor Hotel Reviews]
- **Nama:** Ulfa Stevi Juliana
- **Email:** steviulpa@Gmail.coom
- **ID Dicoding:** MC189D5X2331

#Import library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from tqdm import tqdm
tqdm.pandas()

"""#Data preparation

##Data loading
"""

df = pd.read_csv('tripadvisor_hotel_reviews.csv')
df.head()

"""##Data understanding"""

df.shape

df.isnull().sum()

sns.countplot(data=df,x='Rating')

"""#Encoding label sentimen"""

def label_encode(x):
    if x == 1 or x == 2 or x == 3 :
        return 0
    if x == 4 or x == 5:
        return 1

def label_name(x):
    if x == 0:
        return "Negative"
    if x == 1:
        return "Positive"

df["sentiment"] = df["Rating"].apply(lambda x: label_encode(x))
df["sentiment_name"] = df["sentiment"].apply(lambda x: label_name(x))

df.head()

"""#Data Preprosessing"""

!pip install emoji

import re
import emoji
import string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Inisialisasi stopwords dan stemmer
stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

def clean_text(text):
    # 1. Remove HTML tags
    text = re.sub(r'<.*?>', '', text)

    # 2. Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # 3. Convert emojis to text (e.g., ðŸ˜ƒ -> :smile:)
    text = emoji.demojize(text)

    # 4. Remove digits
    text = re.sub(r'\d+', '', text)

    # 5. Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # 6. Remove stopwords
    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)

    # 7. Stemming
    text = ' '.join(ps.stem(word) for word in text.split())

    return text

df['Review'] = df['Review'].apply(clean_text)
df.head()

"""#Data Spliting 70/30"""

X = df.drop(['sentiment', 'sentiment_name', 'Rating'], axis=1)
y = df['sentiment']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1,stratify=y)
X_train.shape,X_test.shape,y_train.shape,y_test.shape

"""#Tokenization with BERT Tokenizer"""

!pip install transformers
!pip install datasets

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Ambil kolom teks
train_texts = X_train['Review'].tolist()
test_texts = X_test['Review'].tolist()

# Tokenisasi dan padding
train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding=True,
    max_length=128,
    return_tensors='pt'
)

test_encodings = tokenizer(
    test_texts,
    truncation=True,
    padding=True,
    max_length=128,
    return_tensors='pt'
)

# Konversi label ke tensor
train_labels = torch.tensor(y_train.values)
test_labels = torch.tensor(y_test.values)

"""#Model Training"""

from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler

# Buat TensorDataset
train_dataset = TensorDataset(
    train_encodings['input_ids'],
    train_encodings['attention_mask'],
    train_labels
)

test_dataset = TensorDataset(
    test_encodings['input_ids'],
    test_encodings['attention_mask'],
    test_labels
)

# Buat DataLoader
train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)
val_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)

from transformers import BertForSequenceClassification
from torch.optim import AdamW

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=5e-5)

from tqdm import tqdm

epochs = 3
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []

for epoch in range(epochs):
    # Training phase
    model.train()
    total_train_loss = 0
    correct_train_predictions = 0
    total_train_samples = 0

    for batch in tqdm(train_loader):
        optimizer.zero_grad()

        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        logits = outputs.logits

        loss.backward()
        optimizer.step()

        total_train_loss += loss.item()
        predictions = torch.argmax(logits, dim=-1)
        correct_train_predictions += (predictions == labels).sum().item()
        total_train_samples += labels.size(0)

    train_losses.append(total_train_loss / len(train_loader))
    train_accuracies.append(correct_train_predictions / total_train_samples)

    # Validation phase
    model.eval()
    total_val_loss = 0
    correct_val_predictions = 0
    total_val_samples = 0

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch[0].to(device)
            attention_mask = batch[1].to(device)
            labels = batch[2].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_val_loss += loss.item()
            predictions = torch.argmax(logits, dim=-1)
            correct_val_predictions += (predictions == labels).sum().item()
            total_val_samples += labels.size(0)

    val_losses.append(total_val_loss / len(val_loader))
    val_accuracies.append(correct_val_predictions / total_val_samples)

    print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, "
          f"Train Accuracy: {train_accuracies[-1]:.4f}, "
          f"Val Loss: {val_losses[-1]:.4f}, Val Accuracy: {val_accuracies[-1]:.4f}")

"""#Model Evaluation"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Put model in evaluation mode
model.eval()

# Akurasi pada test set
correct_predictions = 0
total_predictions = 0

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        logits = outputs.logits

        predictions = torch.argmax(logits, dim=-1)
        correct_predictions += (predictions == labels).sum().item()
        total_predictions += labels.size(0)

test_accuracy = correct_predictions / total_predictions
print(f"Test Accuracy: {test_accuracy:.4f}")

# Kumpulkan semua prediksi dan label
all_predictions = []
all_labels = []

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=-1)

        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

from sklearn.metrics import classification_report, confusion_matrix

# Classification report
print("Classification Report:")
print(classification_report(all_labels, all_predictions, target_names=["Negative", "Positive"]))

# Confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Negative", "Positive"],
            yticklabels=["Negative", "Positive"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#Model Testing"""

def predict_sentiment(text):
    model.eval()
    inputs = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors="pt")
    inputs = {key: val.to(device) for key, val in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
        prediction = torch.argmax(outputs.logits, dim=-1).item()

    # Ubah label sesuai dataset kamu
    return ["Negative", "Positive"][prediction]

# Sample English reviews for testing
test_texts = [
    "The service at the hotel was terrible. I'm very disappointed.",
    "The room was clean and comfortable. I had a great stay!",
    "Very satisfied with the quality. Will buy again!",
    "The app works smoothly and is very user-friendly.",
    "It was not what I expected. Definitely not worth the price.",
    "The app keeps crashing and is full of bugs.",
    "I waited two weeks and still havenâ€™t received my order.",
    "Poor quality and bad packaging.",
    "Waste of money. I wonâ€™t buy from this store again.",
]

# Predict sentiment for each review
for text in test_texts:
    sentiment = predict_sentiment(text)
    print(f"Review: {text}\nPredicted Sentiment: {sentiment}\n")